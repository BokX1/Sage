# =============================================================================
# Sage Discord Bot - Environment Configuration
# =============================================================================
# IMPORTANT: Never commit this file with real values!
# Copy to .env and fill in your actual values.
# =============================================================================

# -----------------------------------------------------------------------------
# Core / Discord Configuration (REQUIRED)
# -----------------------------------------------------------------------------
NODE_ENV=production
DISCORD_TOKEN=YOUR_DISCORD_TOKEN_HERE
DISCORD_APP_ID=YOUR_APP_ID_HERE
DATABASE_URL="postgresql://postgres:password@localhost:5432/sage?schema=public"
DEV_GUILD_ID=

# -----------------------------------------------------------------------------
# LLM Configuration
# -----------------------------------------------------------------------------
LLM_PROVIDER=pollinations
LLM_BASE_URL=https://gen.pollinations.ai/v1
CHAT_MODEL=openai-large
# Note: Sage uses agent-aware model policy (for example, qwen-coder for coding and perplexity-fast for search).
LLM_IMAGE_BASE_URL=https://gen.pollinations.ai
# Optional: set a global key for higher limits or leave blank and use /sage key set per server
# [NOTE] If left blank, servers MUST provide their own key (BYOP) via /sage key set.
# Get a key at https://pollinations.ai/ or their Discord.
LLM_API_KEY=

# Profile Memory LLM Override
PROFILE_PROVIDER=
PROFILE_CHAT_MODEL=deepseek
PROFILE_UPDATE_INTERVAL=5

# Channel Summary Model
SUMMARY_MODEL=openai-large

# Formatter (reliable JSON)
FORMATTER_MODEL=qwen-coder

# Encryption key for at-rest API key encryption (64 hex chars).
# If you use the onboarding wizard, this value is auto-generated.
# Manual setup: openssl rand -hex 32
SECRET_ENCRYPTION_KEY=

# -----------------------------------------------------------------------------
# Message Storage / Ingestion
# -----------------------------------------------------------------------------
INGESTION_ENABLED=true
INGESTION_MODE=all
INGESTION_ALLOWLIST_CHANNEL_IDS_CSV=
INGESTION_BLOCKLIST_CHANNEL_IDS_CSV=
MESSAGE_DB_STORAGE_ENABLED=true
PROACTIVE_POSTING_ENABLED=true
RAW_MESSAGE_TTL_DAYS=3
RING_BUFFER_MAX_MESSAGES_PER_CHANNEL=200
CONTEXT_TRANSCRIPT_MAX_MESSAGES=15
CONTEXT_TRANSCRIPT_MAX_CHARS=24000

# -----------------------------------------------------------------------------
# Channel Summaries
# -----------------------------------------------------------------------------
SUMMARY_ROLLING_WINDOW_MIN=60
SUMMARY_ROLLING_MIN_MESSAGES=20
SUMMARY_ROLLING_MIN_INTERVAL_SEC=300
SUMMARY_PROFILE_MIN_INTERVAL_SEC=21600
SUMMARY_MAX_CHARS=1800
SUMMARY_SCHED_TICK_SEC=60

# -----------------------------------------------------------------------------
# Bot Behavior
# -----------------------------------------------------------------------------
LOG_LEVEL=info
RATE_LIMIT_MAX=5
RATE_LIMIT_WINDOW_SEC=10
AUTOPILOT_MODE=manual
WAKE_WORDS_CSV=sage
# Optional: prefixes that can precede wake words (e.g., "hey sage")
# Leave empty to only trigger on wake word at start of message
WAKE_WORD_PREFIXES_CSV=
WAKEWORD_COOLDOWN_SEC=10
WAKEWORD_MAX_RESPONSES_PER_MIN_PER_CHANNEL=6

# -----------------------------------------------------------------------------
# Context Budgets
# -----------------------------------------------------------------------------
CONTEXT_MAX_INPUT_TOKENS=120000
CONTEXT_RESERVED_OUTPUT_TOKENS=12000
SYSTEM_PROMPT_MAX_TOKENS=12000
TOKEN_ESTIMATOR=heuristic
TOKEN_HEURISTIC_CHARS_PER_TOKEN=4
CONTEXT_BLOCK_MAX_TOKENS_TRANSCRIPT=20000
CONTEXT_BLOCK_MAX_TOKENS_ROLLING_SUMMARY=12000
CONTEXT_BLOCK_MAX_TOKENS_PROFILE_SUMMARY=12000
CONTEXT_BLOCK_MAX_TOKENS_MEMORY=12000
CONTEXT_BLOCK_MAX_TOKENS_REPLY_CONTEXT=8000
CONTEXT_BLOCK_MAX_TOKENS_PROVIDERS=12000

CONTEXT_USER_MAX_TOKENS=60000
CONTEXT_TRUNCATION_NOTICE=true

# -----------------------------------------------------------------------------
# Agentic Runtime / Tracing
# -----------------------------------------------------------------------------
TRACE_ENABLED=true
AGENTIC_GRAPH_PARALLEL_ENABLED=true
AGENTIC_GRAPH_MAX_PARALLEL=2
AGENTIC_TOOL_ALLOW_EXTERNAL_WRITE=false
AGENTIC_TOOL_ALLOW_HIGH_RISK=false
AGENTIC_TOOL_BLOCKLIST_CSV=join_voice_channel,leave_voice_channel
AGENTIC_CANARY_ENABLED=true
AGENTIC_CANARY_PERCENT=100
AGENTIC_CANARY_ROUTE_ALLOWLIST_CSV=chat,coding,search,creative
AGENTIC_CANARY_MAX_FAILURE_RATE=0.20
AGENTIC_CANARY_MIN_SAMPLES=50
AGENTIC_CANARY_COOLDOWN_SEC=300
AGENTIC_CANARY_WINDOW_SIZE=250
# Optional per-tenant agentic overrides.
# Example:
# AGENTIC_TENANT_POLICY_JSON={"default":{"maxParallel":2},"guilds":{"123":{"allowedModels":["openai-fast","deepseek"]}}}
AGENTIC_TENANT_POLICY_JSON={}
AGENTIC_CRITIC_ENABLED=true
AGENTIC_CRITIC_MIN_SCORE=0.78
AGENTIC_CRITIC_MAX_LOOPS=1

# Replay quality gate controls (used by npm run agentic:replay-gate)
REPLAY_GATE_LIMIT=200
REPLAY_GATE_MIN_AVG_SCORE=0.65
REPLAY_GATE_MIN_SUCCESS_RATE=0.75
REPLAY_GATE_REQUIRE_DATA=1
REPLAY_GATE_MIN_TOTAL=10
REPLAY_GATE_REQUIRED_ROUTES_CSV=chat,coding,search,creative
REPLAY_GATE_MIN_ROUTE_SAMPLES=1
REPLAY_GATE_GUILD_ID=
REPLAY_GATE_CHANNEL_ID=

# Replay seed controls (used by npm run agentic:seed-replay-data)
REPLAY_SEED_PER_ROUTE=3
REPLAY_SEED_GUILD_ID=
REPLAY_SEED_CHANNEL_PREFIX=seed-replay
REPLAY_SEED_USER_ID=seed-user



# -----------------------------------------------------------------------------
# Admin Access Control
# -----------------------------------------------------------------------------
ADMIN_ROLE_IDS_CSV=
ADMIN_USER_IDS_CSV=

# -----------------------------------------------------------------------------
# Timeouts (ms)
# -----------------------------------------------------------------------------
TIMEOUT_CHAT_MS=300000
TIMEOUT_MEMORY_MS=600000

# -----------------------------------------------------------------------------
# Context Budgeting Advanced
# -----------------------------------------------------------------------------
LLM_MODEL_LIMITS_JSON=""

# -----------------------------------------------------------------------------
# Doctor Utility
# -----------------------------------------------------------------------------
# Set to 1 to enable LLM ping in npm run doctor
LLM_DOCTOR_PING=0
